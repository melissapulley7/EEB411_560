---
title: "Lab 3: Part 1"
author: "Dr. Alejandro Molina-Moctezuma;"
date: "2024-02-06"
output: pdf_document
header-includes:
  - \usepackage{awesomebox}
  - \usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}
---



```{r global.options, include = F}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 10,       # the width for plots created by code chunk
    fig.height  = 10,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```




\notebox{You will need to submit the CSV file, and your code. Please answer the questions by annotating your answers in the code (using the pound \# symbol)}

\importantbox{These boxes will inform you of things you need to submit or questions you need to answer!}




## 1 Introduction

The past lectures might have been pretty complicated for some of you. That's OK! Hopefully, this lab will help you.

Before we start, lets looks at some useful definitions

Let's start working on defining probability distributions.

A **distribution** describes the spread of the data which shows all possible values or intervals of the data and how they occur. A population has a distribution, and so does a sample. There are also theoretical distributions. They are also called **probability distributions**. Remember that we talked about random processes, random events and random variables? They all talk about situations (or variables) in which the outcome can not be determined in advance. We can't solve for them, because it can take various values. For example, let's roll a dice.

To roll a 6 side dice, all sides with the same probability of appearing, run the following code:

```{r eval=F}
sample(1:6,1)
```
Run it multiple times. Each time you a rolling a dice, and getting a different value! You don't know what value you're going to get. But you do know that each time, you have a 1/6 probability of obtaining each particular number.
There is a probability distribution that is reigning this. 

Run the following code:
```{r eval =F}
x<-1:6
P<- rep(1/6,6)
df<-data.frame(x,P)
df
```
P represents P(X=x). And what you just did is a theoretical or probability distribution. It's not based on real data, but based on a function that describes the probability of different possible values of a random variable. In this case 1 to 6.

In case you are curious, when talking about a dice roll, we are working with a multinomial distribution (because there are more than 2 possible results). 

## 2. Defining a discrete probability distribution

During Monday's lecture, we looked at defining a discrete probability distribution. Remember? slide 15 has the steps to define a discrete probability distribution. The four steps were:

1.    Define what is the random variable X (uppercase!)
2.    List all possible random variable values x (lowercase!). This is the sample space
3.    List all probability of occurrence for each x. 



So, let's work through this in R. This is the same example used in slide 15! Let's call this scenario 1.


First, let's define the random variable X. In this case, X is the number of heads out of 3 coin tosses (s)

Second, list all possible random variables x. This is the sample space. If we toss a coin 3 times, we can get zero, one, two, or three heads, so, x should be a vector with those 4 values. So, let's write some code that lists all these four potential values. Let's also define n, which is the number of tosses (3).
Finally, let's also define p. This is a fair coin, so the probability is 0.5

```{r eval =F}
x<-
n<-
p<-
```
You will need to fill the right side of the code. Think about how to do this. As we move forward I will be providing less of the code.

Third, we will list the probability of ocurrence for each x.

For this, we use the following:

```{r eval =F}
pmf1<-dbinom(x,n,p)
```
Remember, PMF stands for probability mass function: a probability measure that gives us probabilities of the possible values for a random variable

Let's make a dataframe!
```{r eval = FALSE}
df1<-data.frame(x,p=pmf1)

```

Let's look at our probability distribution!

```{r eval = FALSE}
print(df1)

```

This should be identical to the table we saw in class (slide 15).

Let's obtain the expected value! 
```{r eval=FALSE}
EV<-sum(pmf1*x)
```

The expected value is the sum of the multiplication of each x by their corresponding probability of occurrence, 

Finally, let's plot this.

```{r eval =F}
newplot<-barplot(pmf1, col = "steelblue",names=0:3, ylim=c(0,0.5),main="my title")
text(x=newplot, y=pmf1+0.05,labels = pmf1)
```

If you did everything correct, you should get something that looks like:
```{r echo =F}
x<-0:3
n<-3
p<-0.5

pmf1<-dbinom(x,n,p)
newplot<-barplot(pmf1, col = "steelblue",names=0:3, ylim=c(0,0.5), main="PMF 50% chance of heads")
text(x=newplot, y=pmf1+0.05,labels = pmf1)

```
If you got something different, trace back and see where you might have stumbled.

----

**Some information to dive a little deeper in the binomial probability distribution** 

Feel free to skip the information between the breaklines. However, there is an opportunity for an extra credit. I recommend you don't attempt the extra credit unless you're done with the rest of the lab. It might be a better way to manage your time.

Remember that the probability distribution of a binomial random variable is defined as (equation 1):


$$P(X=x) = \binom{n}{x}p^{x}(1-p)^{n-x}$$
and (equation 2):
$$\binom{n}{x} = \frac{n!}{x!(n-x)!}$$
which is giving us the combination of ways that a certain event might happen. So, in the scenario in which we are tossing 3 coins, the ways we can obtain 2 heads are: HHT, HTH, THH. So, 3. Therefore, if we solved the combinations equation we would get:

$$\binom{3}{2} = 3$$

This is called a combination. As you can tell, the order didn't matter. HHT, HTH, and THH were all counted as "two successes" independently of the order of the elements.

When we care about the order, then it's called a permutation.

\awesomebox[violet]{5pt}{\faRocket}{violet}{EXTRA CREDIT (1 pt) Obtain the probability distribution for scenario 1. But instead of using the dbinom() function, use equation 1 and write the equation (in r) for each of the four possible outcomes. Again, I recommend you come back to this question at the end of the lab}



---

Now, imagine the following scenario (scenario 2):
  
1-     5 coin tosses    

2-     probability of a head is 75%   


\importantbox{Q1. 2 pts. Write code to obtain the PMF, expected value and plot it (barplot) the same way we did it with the 3 coin tosses example. No need to upload the plot, but include the code. Look at the PMF, does it make sense? Why or why not?}


## 3. Defining a discrete probability distribution with your own example

As part of your homework, you were tasked with coming up with a biological example (check Canvas announcement for the parameters of the example). You should have a value for n, a value for p, and x should be 0:n. While you are free to make up these numbers, hopefully they make some biological sense (maybe something you know or something you read, maybe you even found the values in a paper)

We are going to work with the example.

First off, let's define the define the discrete probability function for your example. Follow the instructions that we used for scenario  and scenario 2.

\importantbox{Q2. 3 pts. Describe your biological scenario. Write code to obtain the PMF of your scenario, expected value, and plot it with a barplot. Depending on the size of your plot, displaying the values over the barplot might not be possible or look good. You need to come up with a barplot that you think represents the PMF of your scenario the best. After looking at the barplot, describe the biological importance of that theoretical distribution. Essentially, imagine someone looked at that plot and asked: "but what does it mean?", and they don't have much statistical knowledge. How would you explain that to them?}

A good way to know whether your PMF might be ok is by running:

```{r eval=F}
sum(dbinom(x,n,p))
```

If it equals 1, then we are good!

Now, that PMF shows us the probability of having x successes. Where x = 0, x= 1, x = 2 ... x =n. 

Now, let's simulate a single study. 

This is based on your scenario.
```{r eval = F}
rbinom(n,1,p)

```
What you should be getting is a vector of 1's and 0's. Where 1 is a success and 0 is a failure (I prefer to call that a non-success). Run that code multiple times, and see how the results are different every time (again, a random process!) 

Let's save that simulation in an object called sim1.

To obtain the total number of successes of your simulated data, you can run:
```{r eval = F}

sum(sims1)

```

## 4. Reviewing Bernoulli PMF and Bernoulli Distributions

Once again, I will be defining some important concepts. Hopefully they are clear by now. If by the end of the lab you don't understand them, come talk to me please, so that you won't fall behind.

**PMF (again):** The PMF or probability mass function tells us the probability of getting a specific value of a discrete random variable. It is written as P(X=x), which reads, the probability of a discrete random variable named ‘X’, getting the value ‘x’. The possible values x would depend on the distribution type: Bernoulli would be different from Binomial, and both would be different from Poisson and Negative Binomial. For example, x can only take on possible values of 0 or 1 for a Bernoulli distributed X. By contrast, for Binomial, x can take on possible integer values from 0,1,2…N where N is the # of trials.

**Bernoulli distribution**

A random variable X generated from a Bernoulli distribution with probability of success p can be expressed as: 

$$X \sim {\sf Bernoulli}(x)$$


The PMF of a Bernoulli random variable X is: 

$$ P(X=x) = p^x(1-p)^{1-x} $$
where x can be 0 (failure) or 1 (success). If you substitute x with 1, then:

$$ P(X=1) = p $$
$$ P(X=0) = 1-p $$
## Estimating Maximum Likelihood

I hope you read about MLE (Maximum Likelihood estimates). The "Mark: A gentle introduction" book is a great resource!
For this simulated example, the MLE of *p* is simply the sum of X (or sum of sims1) divided by the total number of observations of X.

For example, $X \sim {\sf Bernoulli}(x)$ and 10 observations of X generated from a probability distribution are:
0 1 1 0 1 0 1 0 1 1

The maximum likelihood estimate (mle) of p (let's call this $\hat{p}$) is $\frac{\sum(X_i)}{n}$, so, in my example:

$$\hat{p} = \frac{0 +1+ 1+ 0+ 1+ 0+ 1+ 0+ 1+ 1}{10} = 0.6$$
Does this make any sense? Is it intuitive?

\importantbox{Q3. 2 pts. Based on your research on maximum likelihood, does it make sense that the mle is 0.6? Explain why you think mle in this case matches with the presented equation }

\importantbox{Q4. 2 pts. Based on your simulated data, estimate the mle for your scenario }

## 5 Poisson distribution

Download the insects csv file and save it in a new object called dat.

This is data on counts of the number of distinct species found in conifer trees in Nicaragua. Each observation is one tree that was sampled for one hour, and the count is the number of species. 

Look at the data. Do you think this is Poisson, or negative binomial (where there is oversidpersion)?

\importantbox{Q5. 1 pts. Estimate the mean, variance and IQR for this count data }

Based on the data (overdispersion?) Think whether it is Poisson, or Negative binomial.

We can explore the distribution of number of species by plotting a histogram:
```{r eval=F}
hist(dat$count,breaks = seq(0,max(dat$count)+5),xlim=c(0,max(dat$count)+5),ylim=c(0,25))
```

What if we generate a random sample of quantities generated from a Poisson distribution with lambda=mean(dat$count)? If the observed species abundance is drawn from a Poisson distribution with lambda equals to its mean, the histogram of random Poisson quantities should look similar to the histogram of observed species abundance.

```{r eval=F}
X_pois <- rpois(n=length(dat$count), lambda=mean(dat$count))

hist(X_pois,breaks = seq(0,max(dat$count)+5),xlim=c(0,max(dat$count)+5))
```

And if we want to plot the two together:

```{r eval = F}
c1 <- rgb(0,102,204,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,255,0, max = 255, alpha = 80, names = "lt.yellow")

hist(dat$count,breaks = seq(0,max(dat$count)+5),xlim=c(0,max(dat$count)+5),col=c1,ylim=c(0,25))


X_pois <- rpois(n=length(dat$count), lambda=mean(dat$count))
hist(X_pois,breaks = seq(0,max(dat$count)+5),xlim=c(0,max(dat$count)+5),ylim=c(0,25),col=c2,add=T)
```

It is good practice to do it multiple times, each time running the rpois code line. 

\importantbox{Q6. 2 pts. Save an overlapping histogram and upload it to Canvas. Explain why running the code multiple times is good practice, and whether you think the Poisson distribution fits based on the overlapping histograms}


***Resources***

*These are hyperlinks*

\href{https://iqss.github.io/dss-workshops/R/Rintro/base-r-cheat-sheet.pdf}{The base R cheat-sheet}


\href{https://www.youtube.com/watch?v=is1tM-eCqXo}{Dr. Fordyce Poisson}


\href{https://cran.r-project.org/doc/manuals/R-intro.pdf}{An introduction to R}

Mark: a Gentel Introduction

\begin{center} End of document \end{center}
